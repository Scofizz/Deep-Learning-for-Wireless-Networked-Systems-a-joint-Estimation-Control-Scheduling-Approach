{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import statistics\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ENV_NAME List of MuJoCo Tasks'''\n",
    "# InvertedDoublePendulum-v2\n",
    "# Hopper-v2\n",
    "# HalfCheetah-v2\n",
    "\n",
    "\n",
    "ENV_NAME = 'HalfCheetah-v2'\n",
    "SEED = 0\n",
    "\n",
    "# Set environment & seeds\n",
    "env = gym.make(ENV_NAME)\n",
    "env.seed(SEED)\n",
    "env.action_space.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set state-action dimension\n",
    "PLANT_STATE_DIM = env.observation_space.shape[0]\n",
    "CHANNEL_STATE_DIM = 1\n",
    "AoI_DIM = 1\n",
    "\n",
    "CONTROL_ACTION_DIM = env.action_space.shape[0]\n",
    "SCHEDULE_ACTION_DIM = 1 \n",
    "MAX_ACTION = float(env.action_space.high[0])\n",
    "\n",
    "# Transmission scheduler action: 1-transmit; 0-not transmit\n",
    "TRANSMISSION_ACTION_LIST = np.array([0,1])\n",
    "\n",
    "# Set NNs hyperparameters of estimator, intelligent controller (actor, critic) and transmission scheduler (DQN)\n",
    "ACTOR_INPUT_DIM = PLANT_STATE_DIM + 2*CHANNEL_STATE_DIM + AoI_DIM # (h,s) → ACT → a\n",
    "ACTOR_OUTPUT_DIM = CONTROL_ACTION_DIM\n",
    "\n",
    "CRITIC_INPUT_DIM = (PLANT_STATE_DIM + 2*CHANNEL_STATE_DIM + AoI_DIM) + CONTROL_ACTION_DIM\n",
    "CRITIC_OUTPUT_DIM = 1\n",
    "\n",
    "ESTIMATOR_INPUT_DIM = PLANT_STATE_DIM + CONTROL_ACTION_DIM\n",
    "ESTIMATOR_OUTPUT_DIM = PLANT_STATE_DIM\n",
    "\n",
    "DQN_INPUT_DIM = PLANT_STATE_DIM + CHANNEL_STATE_DIM + AoI_DIM\n",
    "DQN_OUTPUT_DIM = 2\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "\n",
    "# Set history length\n",
    "LEN = 3\n",
    "\n",
    "SCHEDULER_OW_DIM = LEN*(DQN_INPUT_DIM+SCHEDULE_ACTION_DIM)\n",
    "CONTROLLER_OW_DIM = LEN*(ACTOR_INPUT_DIM+CONTROL_ACTION_DIM)\n",
    "\n",
    "# Set network training hyperparameters\n",
    "TIMESTEPS_BEFORE_TRAIN = 25e3\n",
    "PRE_TRAINING_TIMESTEPS = 2e5\n",
    "MAX_TIMESTEPS = 2e6\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "LR = 3e-4\n",
    "LR_DYNAMICS = 1e-3\n",
    "EPSILON = 0.9\n",
    "\n",
    "EXPL_NOISE_STD = 0.1\n",
    "TARGET_ACTOR_NOISE_STD = 0.2\n",
    "TARGET_ACTOR_NOISE_CLIP = 0.5\n",
    "ACTOR_UPDATE_DELAY = 2\n",
    "\n",
    "MEMORY_CAPACITY = int(2e6)\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "EVAL_EPISODES = 10\n",
    "EVAL_FREQ = 5e3\n",
    "\n",
    "# Set WNCS hyperparameters\n",
    "LS = 0.05 # initial uplink (sensor-controller) channel loss rate\n",
    "LA = 0.05 # initial downlink (controller-actuator) channel loss rate\n",
    "SENSOR_NOISE_STD = 0.01 \n",
    "COMM_COST = 10\n",
    "\n",
    "# Set experience replay hyperparameters\n",
    "ALPHA_IS = 1 # rank-based importance sampling\n",
    "ALPHA_UN = 0 # uniform sampling\n",
    "BETA = 1\n",
    "\n",
    "MEMORY_SIZE = int(2e5)\n",
    "SORT_FREQ = int(2e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initialize NNs'''\n",
    "# Estimator\n",
    "# Intelligent controller (actor, critic) \n",
    "# Transmission scheduler (DQN) \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Estimator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Estimator, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(ESTIMATOR_INPUT_DIM, HIDDEN_SIZE)\n",
    "        self.l1 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.l2 = nn.Linear(HIDDEN_SIZE, ESTIMATOR_OUTPUT_DIM)\n",
    "        \n",
    "\n",
    "    def forward(self, state, action):\n",
    "        self.gru.flatten_parameters()\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        \n",
    "        s_gru, _ = self.gru(sa.view(len(sa), 1, -1))\n",
    "        s = s_gru.view(len(sa), -1)\n",
    "        \n",
    "        s = F.relu(self.l1(s))\n",
    "        \n",
    "        return self.l2(s)\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.pre = nn.Linear(CONTROLLER_OW_DIM, HIDDEN_SIZE)\n",
    "        self.gru = nn.GRU(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        \n",
    "        self.l1 = nn.Linear(ACTOR_INPUT_DIM, HIDDEN_SIZE)\n",
    "        \n",
    "        self.l2 = nn.Linear(2*HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.l3 = nn.Linear(HIDDEN_SIZE, ACTOR_OUTPUT_DIM)\n",
    "\n",
    "        self.max_action = MAX_ACTION\n",
    "\n",
    "\n",
    "    def forward(self, ow, state):\n",
    "        self.gru.flatten_parameters()\n",
    "        \n",
    "        a_his = F.relu(self.pre(ow))\n",
    "        a_gru, _ = self.gru(a_his.view(len(ow), 1, -1))\n",
    "        a_his = a_gru.view(len(ow), -1)\n",
    "        \n",
    "        a_cur = F.relu(self.l1(state))\n",
    "        \n",
    "        a = torch.cat([a_his, a_cur], -1)\n",
    "        a = F.relu(self.l2(a))\n",
    "        \n",
    "        return self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1\n",
    "        self.pre1 = nn.Linear(CONTROLLER_OW_DIM, HIDDEN_SIZE)\n",
    "        self.gru1 = nn.GRU(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        \n",
    "        self.l1 = nn.Linear(CRITIC_INPUT_DIM, HIDDEN_SIZE)\n",
    "        \n",
    "        self.l2 = nn.Linear(2*HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.l3 = nn.Linear(HIDDEN_SIZE, CRITIC_OUTPUT_DIM)\n",
    "\n",
    "        # Q2\n",
    "        self.pre2 = nn.Linear(CONTROLLER_OW_DIM, HIDDEN_SIZE)\n",
    "        self.gru2 = nn.GRU(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        \n",
    "        self.l4 = nn.Linear(CRITIC_INPUT_DIM, HIDDEN_SIZE)\n",
    "        \n",
    "        self.l5 = nn.Linear(2*HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.l6 = nn.Linear(HIDDEN_SIZE, CRITIC_OUTPUT_DIM)\n",
    "        \n",
    "        \n",
    "    def forward(self, ow, state, action):\n",
    "        self.gru1.flatten_parameters()\n",
    "        self.gru2.flatten_parameters()\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        \n",
    "        # Q1\n",
    "        q1_his = F.relu(self.pre1(ow))\n",
    "        q1_gru, _ = self.gru1(q1_his.view(len(ow), 1, -1))\n",
    "        q1_his = q1_gru.view(len(ow), -1)\n",
    "        \n",
    "        q1_cur = F.relu(self.l1(sa))\n",
    "        \n",
    "        q1 = torch.cat([q1_his, q1_cur], -1)\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        \n",
    "        # Q2\n",
    "        q2_his = F.relu(self.pre2(ow))\n",
    "        q2_gru, _ = self.gru2(q2_his.view(len(ow), 1, -1))\n",
    "        q2_his = q2_gru.view(len(ow), -1)\n",
    "        \n",
    "        q2_cur = F.relu(self.l4(sa))\n",
    "        \n",
    "        q2 = torch.cat([q2_his, q2_cur], -1)\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        \n",
    "        return q1, q2\n",
    "\n",
    "\n",
    "    def Q1(self, ow, state, action):\n",
    "        self.gru1.flatten_parameters()\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        \n",
    "        q1_his = F.relu(self.pre1(ow))\n",
    "        q1_gru, _ = self.gru1(q1_his.view(len(ow), 1, -1))\n",
    "        q1_his = q1_gru.view(len(ow), -1)\n",
    "        \n",
    "        q1_cur = F.relu(self.l1(sa))\n",
    "        \n",
    "        q1 = torch.cat([q1_his, q1_cur], -1)\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        \n",
    "        return q1\n",
    "        \n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.pre = nn.Linear(SCHEDULER_OW_DIM, HIDDEN_SIZE)\n",
    "        self.gru = nn.GRU(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        \n",
    "        self.l1 = nn.Linear(DQN_INPUT_DIM, HIDDEN_SIZE)\n",
    "        \n",
    "        self.l2 = nn.Linear(2*HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.l3 = nn.Linear(HIDDEN_SIZE, DQN_OUTPUT_DIM)\n",
    "\n",
    "\n",
    "    def forward(self, ow, state):\n",
    "        self.gru.flatten_parameters()\n",
    "        \n",
    "        q_his = F.relu(self.pre(ow))\n",
    "        q_gru, _ = self.gru(q_his.view(len(ow), 1, -1))\n",
    "        q_his = q_gru.view(len(ow), -1)\n",
    "        \n",
    "        q_cur = F.relu(self.l1(state))\n",
    "        \n",
    "        q = torch.cat([q_his, q_cur], -1)\n",
    "        q = F.relu(self.l2(q))\n",
    "        \n",
    "        return self.l3(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Tailored TD3 Algorithm'''\n",
    "# select_action: generate control signal by actor\n",
    "# select_q: generate Q-value by critic\n",
    "# predict_state: generate predicted state by estimator\n",
    "# schedule_transmission & control_transmission: generate transmission scheduler action based on DQN output\n",
    "# train: network training of estimator, intelligent controller (actor, critic) and transmission scheduler (DQN)\n",
    "\n",
    "\n",
    "class Tailored_TD3(object):\n",
    "    def __init__(self):\n",
    "        self.estimator = Estimator().to(device)\n",
    "        self.estimator_optimizer = torch.optim.Adam(self.estimator.parameters(), lr=LR_DYNAMICS)\n",
    "        \n",
    "        self.dqn = DQN().to(device)\n",
    "        self.dqn_target = copy.deepcopy(self.dqn)\n",
    "        self.dqn_optimizer = torch.optim.Adam(self.dqn.parameters(), lr=LR)\n",
    "\n",
    "        self.actor = Actor().to(device)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=LR)\n",
    "\n",
    "        self.critic = Critic().to(device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=LR)\n",
    "\n",
    "        self.max_action = MAX_ACTION\n",
    "        self.discount = GAMMA\n",
    "        self.tau = TAU\n",
    "        self.policy_noise = TARGET_ACTOR_NOISE_STD\n",
    "        self.noise_clip = TARGET_ACTOR_NOISE_CLIP\n",
    "        self.policy_freq = ACTOR_UPDATE_DELAY\n",
    "\n",
    "        self.total_it = 0\n",
    "\n",
    "\n",
    "    def select_action(self, ow, state):\n",
    "        ow = torch.FloatTensor(ow.reshape(1, -1)).to(device)\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(ow, state).cpu().data.numpy().flatten()\n",
    "    \n",
    "    \n",
    "    def select_q(self, ow, state, action):\n",
    "        ow = torch.FloatTensor(ow.reshape(1, -1)).to(device)\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        action = torch.FloatTensor(action.reshape(1, -1)).to(device)\n",
    "        return self.critic.Q1(ow, state, action).cpu().data.numpy().flatten()\n",
    "\n",
    "    \n",
    "    def predict_state(self, state, action):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        action = torch.FloatTensor(action.reshape(1, -1)).to(device)\n",
    "        return self.estimator(state, action).cpu().data.numpy().flatten()\n",
    "    \n",
    "    \n",
    "    def schedule_transmission(self, ow, state):\n",
    "        ow = torch.FloatTensor(ow.reshape(1, -1)).to(device)\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        \n",
    "        schedule_action_value = self.dqn(ow, state)\n",
    "        schedule_action = torch.max(schedule_action_value, 1)[1].cpu().data.numpy()[0]\n",
    "        return np.array([schedule_action])\n",
    "    \n",
    "    \n",
    "    def control_transmission(self, schedule_action):\n",
    "        transmission_action_idx = int(schedule_action)\n",
    "        transmission_action = TRANSMISSION_ACTION_LIST[transmission_action_idx]\n",
    "        return transmission_action\n",
    "\n",
    "    \n",
    "    def train(self, estimator_replay_buffer, scheduler_replay_buffer, controller_replay_buffer):\n",
    "        # Sort memory every n steps\n",
    "        if(self.total_it % SORT_FREQ == 0):\n",
    "            controller_replay_buffer.sort_priorities()\n",
    "            \n",
    "        self.total_it += 1\n",
    "        \n",
    "        # Sample replay buffer (Estimator update)\n",
    "        state_E, action_E, reward_E, next_state_E, not_done_E, idxs_E, sampling_weights_E = estimator_replay_buffer.sample()\n",
    "        sampling_weights_E_sqrt = sampling_weights_E.sqrt()\n",
    "\n",
    "        # Compute estimator loss\n",
    "        estimator_loss = F.mse_loss(self.estimator(state_E, action_E), next_state_E)\n",
    "\n",
    "        # Optimize the estimator\n",
    "        self.estimator_optimizer.zero_grad()\n",
    "        estimator_loss.backward()\n",
    "        self.estimator_optimizer.step()\n",
    "        \n",
    "        # Sample replay buffer (Actor-critic update)\n",
    "        ow, state, action, reward, next_ow, next_state, not_done, idxs, sampling_weights = controller_replay_buffer.sample_ow()\n",
    "        sampling_weights_sqrt = sampling_weights.sqrt()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy and add clipped noise\n",
    "            noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "            next_action = (self.actor_target(next_ow, next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_ow, next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "        # Get current Q estimates\n",
    "        current_Q1, current_Q2 = self.critic(ow, state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(sampling_weights_sqrt*current_Q1, \n",
    "                                 sampling_weights_sqrt*target_Q) + F.mse_loss(sampling_weights_sqrt*current_Q2, \n",
    "                                                                              sampling_weights_sqrt*target_Q)\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Delayed policy updates\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "\n",
    "            # Compute actor loss\n",
    "            actor_loss = -self.critic.Q1(ow, state, self.actor(ow, state)).mean()\n",
    "\n",
    "            # Optimize the actor \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update target networks\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        # Update sampling priority according to AoI-based experience replay\n",
    "        update_idxs = idxs.cpu().numpy().astype(int)\n",
    "        TD_error = ((current_Q1 - target_Q)**2 + (current_Q2 - target_Q)**2).detach().cpu().numpy().flatten().astype(float)\n",
    "        controller_replay_buffer.update_priorities(update_idxs, TD_error)\n",
    "        \n",
    "        if(self.total_it >= PRE_TRAINING_TIMESTEPS - TIMESTEPS_BEFORE_TRAIN):\n",
    "            # Sample replay buffer (DQN update)\n",
    "            ow_S, state_S, action_S, reward_S, next_ow_S, next_state_S, not_done_S, idxs_S, sampling_weights_S = scheduler_replay_buffer.sample_ow()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Compute the target Q value\n",
    "                target_Q_S = self.dqn_target(next_ow_S, next_state_S)\n",
    "                target_Q_S = reward_S + not_done_S * self.discount * target_Q_S.max(1)[0].unsqueeze(1)\n",
    "\n",
    "            # Get current Q estimates\n",
    "            action_S = action_S.long()\n",
    "            current_Q_S = self.dqn(ow_S, state_S).gather(1, action_S)\n",
    "\n",
    "            # Compute dqn loss\n",
    "            dqn_loss = F.mse_loss(current_Q_S, target_Q_S)\n",
    "\n",
    "            # Optimize the dqn\n",
    "            self.dqn_optimizer.zero_grad()\n",
    "            dqn_loss.backward()\n",
    "            self.dqn_optimizer.step()\n",
    "            \n",
    "            if self.total_it % 100 == 0:\n",
    "                # Update target networks\n",
    "                for param, target_param in zip(self.dqn.parameters(), self.dqn_target.parameters()):\n",
    "                    target_param.data.copy_(param.data)\n",
    "                    \n",
    "            # Update sampling priority according to AoI-based experience replay\n",
    "            update_idxs_S = idxs_S.cpu().numpy().astype(int)\n",
    "            TD_error_S = ((current_Q_S - target_Q_S)**2).detach().cpu().numpy().flatten().astype(float)\n",
    "            scheduler_replay_buffer.update_priorities(update_idxs_S, TD_error_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initialize Experience Replay Buffer'''\n",
    "# PrioritizedExperienceReplayBuffer: transitions in the format of [state, action, reward, next_state, done]\n",
    "# PrioritizedExperienceReplayBuffer_OW: transitions in the format of [history, state, action, reward, next_history, next_state, done]\n",
    "\n",
    "Experience = namedtuple(\"Experience\", (\"states\", \"actions\", \"rewards\", \"next_states\", \"dones\"))\n",
    "Experience_OW = namedtuple(\"Experience_OW\", (\"ows\", \"states\", \"actions\", \"rewards\", \"next_ows\", \"next_states\", \"dones\")) \n",
    "\n",
    "class PrioritizedExperienceReplayBuffer:\n",
    "\n",
    "    def __init__(self, alpha=ALPHA_UN, beta=BETA):\n",
    "      \n",
    "        self._batch_size = BATCH_SIZE\n",
    "        self._buffer_size = MEMORY_CAPACITY\n",
    "        self._buffer_length = 0 # current buffer length\n",
    "        self._buffer = np.empty(self._buffer_size, dtype=[(\"priority\", np.float32), (\"experience\", Experience)])\n",
    "        \n",
    "        self._alpha = alpha\n",
    "        self._beta = beta\n",
    "        self._random_state = np.random.RandomState()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def buffer_len(self):\n",
    "        return self._buffer_length\n",
    "    \n",
    "    def is_full(self) -> bool:\n",
    "        return self._buffer_length == self._buffer_size\n",
    "    \n",
    "    def add(self, experience: Experience, AoI):\n",
    "        priority = -AoI\n",
    "        if self.is_full():\n",
    "            self._buffer = np.insert(self._buffer,self._buffer_size,(priority, experience)) # push to end (newest)\n",
    "            self._buffer = np.delete(self._buffer,0) # pop the first (oldest)\n",
    "        else:\n",
    "            self._buffer[self._buffer_length] = (priority, experience)\n",
    "            self._buffer_length += 1\n",
    "\n",
    "    def sample(self):\n",
    "        # generate rank-based sampling probability sequence\n",
    "        if(self._buffer_length >= MEMORY_SIZE):\n",
    "            rank_ps = np.arange(MEMORY_SIZE)+1\n",
    "        else:\n",
    "            rank_ps = np.arange(self._buffer_length)+1\n",
    "        sampling_probs = rank_ps**self._alpha / np.sum(rank_ps**self._alpha, dtype=np.int64)\n",
    "        # sample transitions \n",
    "        idxs_ps = self._random_state.choice(np.arange(rank_ps.size),\n",
    "                                         size=self._batch_size,\n",
    "                                         replace=True,\n",
    "                                         p=sampling_probs)\n",
    "        \n",
    "        if(self._buffer_length >= MEMORY_SIZE):\n",
    "            idxs = idxs_ps + (self._buffer_length-MEMORY_SIZE)\n",
    "        else:\n",
    "            idxs = idxs_ps\n",
    "        # compute sampling weights\n",
    "        experiences = self._buffer[\"experience\"][idxs]\n",
    "        batch = Experience(*zip(*experiences))\n",
    "        \n",
    "        if(self._buffer_length >= MEMORY_SIZE):\n",
    "            weights = (MEMORY_SIZE * sampling_probs[idxs_ps])**-self._beta\n",
    "        else:\n",
    "            weights = (self._buffer_length * sampling_probs[idxs_ps])**-self._beta\n",
    "        normalized_weights = weights / weights.max()\n",
    "                        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(batch.states)).to(self.device),\n",
    "            torch.FloatTensor(np.array(batch.actions)).to(self.device),\n",
    "            torch.FloatTensor(np.array(batch.rewards)).to(self.device),\n",
    "            torch.FloatTensor(np.array(batch.next_states)).to(self.device),\n",
    "            torch.FloatTensor(np.array(batch.dones)).to(self.device),\n",
    "            torch.FloatTensor(np.array(idxs)).to(self.device),\n",
    "            torch.FloatTensor(np.array(normalized_weights)).to(self.device)\n",
    "        )\n",
    "    \n",
    "\n",
    "class PrioritizedExperienceReplayBuffer_OW:\n",
    "\n",
    "    def __init__(self, alpha=ALPHA_IS, beta=BETA):\n",
    "      \n",
    "        self._batch_size = BATCH_SIZE\n",
    "        self._buffer_size = MEMORY_CAPACITY\n",
    "        self._buffer_length = 0 # current buffer length\n",
    "        self._buffer = np.empty(self._buffer_size, dtype=[(\"priority\", np.float32), (\"experience_ow\", Experience_OW)])\n",
    "        \n",
    "        self._alpha = alpha\n",
    "        self._beta = beta\n",
    "        self._random_state = np.random.RandomState()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def buffer_len(self):\n",
    "        return self._buffer_length\n",
    "    \n",
    "    def is_full(self) -> bool:\n",
    "        return self._buffer_length == self._buffer_size\n",
    "    \n",
    "    def add(self, experience_ow: Experience_OW, AoI):\n",
    "        priority = -AoI\n",
    "        if self.is_full():\n",
    "            self._buffer = np.insert(self._buffer,self._buffer_size,(priority, experience_ow)) # push to end (newest)\n",
    "            self._buffer = np.delete(self._buffer,0) # pop the first (oldest)\n",
    "        else:\n",
    "            self._buffer[self._buffer_length] = (priority, experience_ow)\n",
    "            self._buffer_length += 1\n",
    "\n",
    "    def sample_ow(self):\n",
    "        # generate rank-based sampling probability sequence\n",
    "        if(self._buffer_length >= MEMORY_SIZE):\n",
    "            rank_ps = np.arange(MEMORY_SIZE)+1\n",
    "        else:\n",
    "            rank_ps = np.arange(self._buffer_length)+1\n",
    "        sampling_probs = rank_ps**self._alpha / np.sum(rank_ps**self._alpha, dtype=np.int64)\n",
    "        # sample transitions\n",
    "        idxs_ps = self._random_state.choice(np.arange(rank_ps.size),\n",
    "                                         size=self._batch_size,\n",
    "                                         replace=True,\n",
    "                                         p=sampling_probs)\n",
    "        \n",
    "        if(self._buffer_length >= MEMORY_SIZE):\n",
    "            idxs = idxs_ps + (self._buffer_length-MEMORY_SIZE)\n",
    "        else:\n",
    "            idxs = idxs_ps\n",
    "        # compute sampling weights\n",
    "        experiences_ow = self._buffer[\"experience_ow\"][idxs]\n",
    "        batch = Experience_OW(*zip(*experiences_ow))\n",
    "        \n",
    "        if(self._buffer_length >= MEMORY_SIZE):\n",
    "            weights = (MEMORY_SIZE * sampling_probs[idxs_ps])**-self._beta\n",
    "        else:\n",
    "            weights = (self._buffer_length * sampling_probs[idxs_ps])**-self._beta\n",
    "        normalized_weights = weights / weights.max()\n",
    "                        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(batch.ows)).to(self.device),\n",
    "            torch.FloatTensor(np.array(batch.states)).to(self.device),\n",
    "            torch.FloatTensor(np.array(batch.actions)).to(self.device),\n",
    "            torch.FloatTensor(np.array(batch.rewards)).to(self.device),\n",
    "            torch.FloatTensor(np.array(batch.next_ows)).to(self.device),\n",
    "            torch.FloatTensor(np.array(batch.next_states)).to(self.device),\n",
    "            torch.FloatTensor(np.array(batch.dones)).to(self.device),\n",
    "            torch.FloatTensor(np.array(idxs)).to(self.device),\n",
    "            torch.FloatTensor(np.array(normalized_weights)).to(self.device)\n",
    "        )\n",
    "    \n",
    "    # Update the ranking value of transitions \n",
    "    def update_priorities(self, idxs: np.array, priorities: np.array):\n",
    "        for i in idxs:\n",
    "            self._buffer[\"priority\"][i] = float(math.floor(self._buffer[\"priority\"][i]))\n",
    "        \n",
    "        priorities_sigmoid = (1/(1 + np.exp(-priorities))).round(decimals=4) - 1e-4 # bias for rounding to avoid sigmoid≈1\n",
    "        self._buffer[\"priority\"][idxs] += priorities_sigmoid\n",
    "    \n",
    "    # Sort transitions in the memory based on ranking value\n",
    "    def sort_priorities(self):\n",
    "        if(self._buffer_length >= MEMORY_SIZE):\n",
    "            buffer_list = list(self._buffer[(self._buffer_length - MEMORY_SIZE):self._buffer_length])\n",
    "            buffer_list.sort(key=lambda x:x[0])\n",
    "\n",
    "            self._buffer_length = MEMORY_SIZE\n",
    "        else:\n",
    "            buffer_list = list(self._buffer[:self._buffer_length])\n",
    "            buffer_list.sort(key=lambda x:x[0])\n",
    "        \n",
    "        self._buffer[:self._buffer_length] = np.array(buffer_list, \n",
    "                                                      dtype=[(\"priority\", np.float32), (\"experience_ow\", Experience_OW)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initialize History Buffer'''\n",
    "# Controller_ObservationAction_Window: state-action history of the intelligent controller\n",
    "# Scheduler_ObservationAction_Window: state-action history of the transmission scheduler\n",
    "\n",
    "\n",
    "class Controller_ObservationAction_Window:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._buffer_size = LEN\n",
    "        self._buffer = deque()\n",
    "        # padding zeros\n",
    "        for i in range(LEN):\n",
    "            self._buffer.append(np.zeros(ACTOR_INPUT_DIM))\n",
    "            self._buffer.append(np.zeros(CONTROL_ACTION_DIM))\n",
    "    \n",
    "    def add(self, obervation_OR_action):\n",
    "        self._buffer.append(obervation_OR_action)\n",
    "        self._buffer.popleft()\n",
    "        \n",
    "    def read(self):\n",
    "        his_obs_act = self._buffer.copy()\n",
    "        his_obs_act = np.array(his_obs_act)\n",
    "        his_obs_act = np.concatenate(his_obs_act, axis=None)\n",
    "        return his_obs_act\n",
    "    \n",
    "\n",
    "class Scheduler_ObservationAction_Window:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._buffer_size = LEN\n",
    "        self._buffer = deque()\n",
    "        # padding zeros\n",
    "        for i in range(LEN):\n",
    "            self._buffer.append(np.zeros(DQN_INPUT_DIM))\n",
    "            self._buffer.append(np.zeros(SCHEDULE_ACTION_DIM))\n",
    "    \n",
    "    def add(self, obervation_OR_action):\n",
    "        self._buffer.append(obervation_OR_action)\n",
    "        self._buffer.popleft()\n",
    "        \n",
    "    def read(self):\n",
    "        his_obs_act = self._buffer.copy()\n",
    "        his_obs_act = np.array(his_obs_act)\n",
    "        his_obs_act = np.concatenate(his_obs_act, axis=None)\n",
    "        return his_obs_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Functions for Implementation'''\n",
    "# channel_state_transition: Markov fading channel state\n",
    "# add_gaussian_noise: add sensor measurement noise\n",
    "# concatenate_controller_state: generate input of intelligent controller\n",
    "# concatenate_scheduler_state: generate input of transmission scheduler\n",
    "\n",
    "def channel_state_transition(ch):\n",
    "    if(ch == 0.05):\n",
    "        next_ch = np.random.choice([0.05, 0.1], p=[0.3, 0.7])\n",
    "    else:\n",
    "        next_ch = np.random.choice([0.05, 0.1], p=[0.7, 0.3])\n",
    "    return next_ch\n",
    "\n",
    "def add_gaussian_noise(state):\n",
    "    noise = np.random.randn(state.shape[0]) \n",
    "    sensor = state + SENSOR_NOISE_STD * noise\n",
    "    return sensor\n",
    "\n",
    "def concatenate_controller_state(plant_state, ch_sc, ch_ca, AoI_sc):\n",
    "    ch_state = np.array([ch_sc, ch_ca, AoI_sc], dtype=np.float32)\n",
    "    state = np.concatenate((plant_state,ch_state))\n",
    "    return state\n",
    "\n",
    "def concatenate_scheduler_state(plant_state, ch_sc, AoI_sc):\n",
    "    ch_state = np.array([ch_sc, AoI_sc], dtype=np.float32)\n",
    "    state = np.concatenate((plant_state,ch_state))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluation of trained NNs on MuJoCo tasks'''\n",
    "# Estimator\n",
    "# Intelligent controller (actor, critic) \n",
    "# Transmission scheduler (DQN) \n",
    "\n",
    "\n",
    "def eval_policy(policy, env_name, seed, training_timestep, eval_episodes=EVAL_EPISODES):\n",
    "    eval_env = gym.make(env_name)\n",
    "    eval_env.seed(seed + 100)\n",
    "    \n",
    "    eval_return_buffer = []\n",
    "    eval_error_buffer = []\n",
    "    \n",
    "    eval_scheduler_OW = Scheduler_ObservationAction_Window()\n",
    "    eval_controller_OW = Controller_ObservationAction_Window()\n",
    "    \n",
    "    for i in range(eval_episodes):\n",
    "        eval_episode_reward = 0.\n",
    "        eval_episode_comm_cost = 0.\n",
    "        eval_episode_error = 0.\n",
    "        \n",
    "        eval_timesteps = 0\n",
    "        \n",
    "        eval_state, eval_done = eval_env.reset(), False\n",
    "        EVAL_INITIAL_OBS = add_gaussian_noise(eval_state)\n",
    "        \n",
    "        eval_controller_input = EVAL_INITIAL_OBS\n",
    "        \n",
    "        eval_scheduler_OW.__init__()\n",
    "        eval_controller_OW.__init__()\n",
    "        \n",
    "        eval_LS = 0.05 # initial uplink (sensor-controller) channel loss rate\n",
    "        eval_LA = 0.05 # initial downlink (controller-actuator) channel loss rate\n",
    "        \n",
    "        eval_AoI = 0\n",
    "        \n",
    "        eval_scheduler_concatenated_input = concatenate_scheduler_state(EVAL_INITIAL_OBS, eval_LS, 1)\n",
    "        eval_schedule = np.array([1])\n",
    "        eval_sc_transmission = 1\n",
    "        \n",
    "        # check unusual env state\n",
    "        nan_signal = 0\n",
    "        \n",
    "        while not eval_done:\n",
    "            # check unusual env state\n",
    "            if(nan_signal):\n",
    "                break\n",
    "        \n",
    "            eval_controller_concatenated_input = concatenate_controller_state(eval_controller_input, eval_LS, eval_LA, eval_AoI)\n",
    "            eval_controller_concatenated_ow = eval_controller_OW.read()\n",
    "              \n",
    "            if(eval_sc_transmission == 0):\n",
    "                eval_comm_cost_sc = 0\n",
    "            else:\n",
    "                eval_comm_cost_sc = COMM_COST\n",
    "             \n",
    "            if(np.random.rand() < eval_LA):\n",
    "                eval_action = np.zeros(CONTROL_ACTION_DIM)\n",
    "            else:\n",
    "                eval_action = policy.select_action(np.array(eval_controller_concatenated_ow), \n",
    "                                              np.array(eval_controller_concatenated_input))\n",
    "            \n",
    "            eval_scheduler_OW.add(eval_scheduler_concatenated_input)\n",
    "            eval_scheduler_OW.add(eval_schedule)\n",
    "            \n",
    "            eval_controller_OW.add(eval_controller_concatenated_input)\n",
    "            eval_controller_OW.add(eval_action)\n",
    "            \n",
    "            # check unusual env state\n",
    "            if(True in np.isnan(eval_action)):\n",
    "                nan_signal = 1\n",
    "                break\n",
    "            \n",
    "            # take 1 step\n",
    "            eval_state_est = policy.predict_state(np.array(eval_controller_input), np.array(eval_action))\n",
    "            \n",
    "            eval_state, eval_oc_cost, eval_done, _ = eval_env.step(eval_action)\n",
    "            eval_state_obs = add_gaussian_noise(eval_state)\n",
    "            \n",
    "            eval_reward = eval_oc_cost - eval_comm_cost_sc\n",
    "        \n",
    "            eval_LS = channel_state_transition(eval_LS)\n",
    "            eval_LA = channel_state_transition(eval_LA) \n",
    "           \n",
    "            eval_scheduler_concatenated_input = concatenate_scheduler_state(eval_state_est, eval_LS, eval_AoI+1)\n",
    "            eval_scheduler_concatenated_ow = eval_scheduler_OW.read()\n",
    "            \n",
    "            if(training_timestep > PRE_TRAINING_TIMESTEPS):\n",
    "                eval_schedule = policy.schedule_transmission(np.array(eval_scheduler_concatenated_ow), \n",
    "                                                            np.array(eval_scheduler_concatenated_input))\n",
    "            else:\n",
    "                eval_schedule = np.array([1])\n",
    "            \n",
    "            eval_sc_transmission = policy.control_transmission(eval_schedule)\n",
    "            \n",
    "            # next step\n",
    "            if(np.random.rand() < eval_LS) or (eval_sc_transmission == 0):\n",
    "                eval_controller_input = eval_state_est\n",
    "                eval_AoI += 1\n",
    "            else:\n",
    "                eval_controller_input = eval_state_obs\n",
    "                eval_AoI = 0\n",
    "            \n",
    "            # Evaluate reward\n",
    "            eval_episode_reward += eval_reward\n",
    "            \n",
    "            # Evaluate prediction error\n",
    "            eval_timestep_error = np.square(eval_state_obs - eval_state_est).mean()\n",
    "            eval_episode_error += eval_timestep_error\n",
    "            eval_timesteps += 1\n",
    "        \n",
    "        if not(nan_signal):\n",
    "            eval_return_buffer.append(eval_episode_reward)\n",
    "            eval_error_buffer.append(eval_episode_error / eval_timesteps)\n",
    "    \n",
    "    eval_env.close()\n",
    "    \n",
    "    # Average over N eval episodes\n",
    "    eval_return = statistics.mean(eval_return_buffer)\n",
    "    eval_error = statistics.mean(eval_error_buffer)\n",
    "   \n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {eval_return:.3f}\")\n",
    "    print(f\"Estimation over {eval_episodes} episodes: {eval_error:.3f}\")\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Main: joint training of NNs in WNCS over fading channels'''\n",
    "# Estimator\n",
    "# Intelligent controller (actor, critic) \n",
    "# Transmission scheduler (DQN) \n",
    "\n",
    "\n",
    "# Initialize policy\n",
    "policy = Tailored_TD3()\n",
    "\n",
    "# Initialize replay buffer\n",
    "estimator_replay_buffer = PrioritizedExperienceReplayBuffer()\n",
    "scheduler_replay_buffer = PrioritizedExperienceReplayBuffer_OW(ALPHA_UN, BETA)\n",
    "controller_replay_buffer = PrioritizedExperienceReplayBuffer_OW()\n",
    "\n",
    "# Evaluate untrained policy\n",
    "eval_policy(policy, ENV_NAME, SEED, 0)\n",
    "\n",
    "state, done = env.reset(), False\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "\n",
    "INITIAL_OBS = add_gaussian_noise(state)\n",
    "\n",
    "controller_obs = None\n",
    "controller_est = None\n",
    "\n",
    "controller_next_obs = INITIAL_OBS\n",
    "controller_next_est = None\n",
    "\n",
    "LS = LS # initial uplink (sensor-controller) channel loss rate\n",
    "LA = LA # initial downlink (controller-actuator) channel loss rate\n",
    "\n",
    "signal_sc = 1 # uplink dropout signal (0: drop, 1: receive)\n",
    "AoI = 0\n",
    "\n",
    "# Initialize key variables\n",
    "SCHEDULER_OW = Scheduler_ObservationAction_Window()\n",
    "CONTROLLER_OW = Controller_ObservationAction_Window()\n",
    "\n",
    "scheduler_concatenated_est = concatenate_scheduler_state(INITIAL_OBS, LS, 1)\n",
    "scheduler_concatenated_ow = SCHEDULER_OW.read()\n",
    "schedule = np.array([1]) \n",
    "sc_transmission = 1\n",
    "\n",
    "# Initialize temp variables\n",
    "REC_estimator_current = None\n",
    "REC_estimator_next = None\n",
    "\n",
    "REC_scheduler_current = None\n",
    "REC_scheduler_next = None\n",
    "\n",
    "REC_scheduler_current_ow = None\n",
    "REC_scheduler_next_ow = None\n",
    "\n",
    "REC_controller_current = None\n",
    "REC_controller_next = None\n",
    "\n",
    "REC_controller_current_ow = None\n",
    "REC_controller_next_ow = None\n",
    "\n",
    "for t in range(int(MAX_TIMESTEPS)):\n",
    "\n",
    "    episode_timesteps += 1\n",
    "    \n",
    "    '''Packet dropout signal'''\n",
    "    if (signal_sc == 0):\n",
    "        '''Uplink channel'''\n",
    "        controller_est = controller_next_est\n",
    "        \n",
    "        controller_concatenated_est = concatenate_controller_state(controller_est, LS, LA, AoI)\n",
    "        controller_concatenated_ow = CONTROLLER_OW.read()\n",
    "        \n",
    "        REC_estimator_current = controller_est\n",
    "        \n",
    "        REC_scheduler_current = scheduler_concatenated_est\n",
    "        REC_scheduler_current_ow = scheduler_concatenated_ow\n",
    "        \n",
    "        REC_controller_current = controller_concatenated_est\n",
    "        REC_controller_current_ow = controller_concatenated_ow\n",
    "        \n",
    "        # communication cost    \n",
    "        if(sc_transmission == 0):\n",
    "            comm_cost_sc = 0\n",
    "        else:\n",
    "            comm_cost_sc = COMM_COST\n",
    "        \n",
    "        '''Downlink channel'''\n",
    "        if(np.random.rand() < LA):\n",
    "            action = np.zeros(CONTROL_ACTION_DIM)\n",
    "        else:\n",
    "            # Select action randomly or according to policy\n",
    "            if t < TIMESTEPS_BEFORE_TRAIN:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = (policy.select_action(np.array(controller_concatenated_ow), \n",
    "                                               np.array(controller_concatenated_est)) \n",
    "                          + np.random.normal(0, MAX_ACTION * EXPL_NOISE_STD, \n",
    "                                           size=CONTROL_ACTION_DIM)).clip(-MAX_ACTION, MAX_ACTION)\n",
    "        \n",
    "        # Update history of transmission scheduler and intelligent controller  \n",
    "        SCHEDULER_OW.add(scheduler_concatenated_est)\n",
    "        SCHEDULER_OW.add(schedule)\n",
    "        \n",
    "        CONTROLLER_OW.add(controller_concatenated_est)\n",
    "        CONTROLLER_OW.add(action)\n",
    "        \n",
    "        # Take 1 step\n",
    "        controller_next_est = policy.predict_state(np.array(controller_est), np.array(action))\n",
    "        \n",
    "        next_state, oc_cost, done, _ = env.step(action) \n",
    "        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "        \n",
    "        reward = oc_cost - comm_cost_sc\n",
    "        \n",
    "        q_value = policy.select_q(controller_concatenated_ow, controller_concatenated_est, action)\n",
    "        q_reward = float(q_value)\n",
    "        \n",
    "        '''Markov fading channel state'''\n",
    "        LS = channel_state_transition(LS)\n",
    "        LA = channel_state_transition(LA)\n",
    "        \n",
    "        '''Transmission scheduling'''\n",
    "        scheduler_concatenated_est = concatenate_scheduler_state(controller_next_est, LS, AoI+1)\n",
    "        scheduler_concatenated_ow = SCHEDULER_OW.read()\n",
    "        \n",
    "        if(t < PRE_TRAINING_TIMESTEPS):    \n",
    "            schedule = np.array([1])\n",
    "        else:\n",
    "            # Select schedule action with e-greedy\n",
    "            if(np.random.rand() < EPSILON):\n",
    "                schedule = policy.schedule_transmission(np.array(scheduler_concatenated_ow), \n",
    "                                                        np.array(scheduler_concatenated_est))\n",
    "            else:\n",
    "                schedule = np.array([np.random.randint(0, 2)])\n",
    "        \n",
    "        sc_transmission = policy.control_transmission(schedule)\n",
    "        \n",
    "        '''Next time step'''\n",
    "        REC_scheduler_next = scheduler_concatenated_est\n",
    "        REC_scheduler_next_ow = scheduler_concatenated_ow\n",
    "        \n",
    "        if(np.random.rand() < LS) or (sc_transmission == 0):\n",
    "            # Predict state\n",
    "            controller_next_est = controller_next_est\n",
    "            \n",
    "            AoI += 1\n",
    "            \n",
    "            REC_estimator_next = controller_next_est\n",
    "            REC_controller_next = concatenate_controller_state(controller_next_est, LS, LA, AoI)\n",
    "            REC_controller_next_ow = CONTROLLER_OW.read()\n",
    "            \n",
    "            # Store transition in replay buffers\n",
    "            scheduler_experience = Experience_OW(REC_scheduler_current_ow, REC_scheduler_current, schedule, [q_reward], \n",
    "                                       REC_scheduler_next_ow, REC_scheduler_next, [1. - done_bool])\n",
    "            controller_experience = Experience_OW(REC_controller_current_ow, REC_controller_current, action, [reward], \n",
    "                                       REC_controller_next_ow, REC_controller_next, [1. - done_bool])\n",
    "            \n",
    "            aoi_label = 2*AoI - 1\n",
    "            scheduler_replay_buffer.add(scheduler_experience, aoi_label) # transition AOI: n/n+1\n",
    "            controller_replay_buffer.add(controller_experience, aoi_label)\n",
    "        else:  \n",
    "            # Receive noisy observation\n",
    "            controller_next_obs = add_gaussian_noise(next_state)\n",
    "            \n",
    "            signal_sc = 1\n",
    "            \n",
    "            REC_estimator_next = controller_next_obs\n",
    "            REC_controller_next = concatenate_controller_state(controller_next_obs, LS, LA, 0)\n",
    "            REC_controller_next_ow = CONTROLLER_OW.read()\n",
    "            \n",
    "            # Store transition in replay buffers\n",
    "            scheduler_experience = Experience_OW(REC_scheduler_current_ow, REC_scheduler_current, schedule, [q_reward], \n",
    "                                       REC_scheduler_next_ow, REC_scheduler_next, [1. - done_bool])\n",
    "            controller_experience = Experience_OW(REC_controller_current_ow, REC_controller_current, action, [reward], \n",
    "                                          REC_controller_next_ow, REC_controller_next, [1. - done_bool])\n",
    "            \n",
    "            aoi_label = AoI\n",
    "            scheduler_replay_buffer.add(scheduler_experience, aoi_label) # transition AOI: n/0\n",
    "            controller_replay_buffer.add(controller_experience, aoi_label)\n",
    "    \n",
    "    else: \n",
    "        # Reset AoI\n",
    "        AoI = 0\n",
    "        \n",
    "        '''Uplink channel'''\n",
    "        controller_obs = controller_next_obs\n",
    "        \n",
    "        controller_concatenated_obs = concatenate_controller_state(controller_obs, LS, LA, 0)\n",
    "        controller_concatenated_ow = CONTROLLER_OW.read()\n",
    "        \n",
    "        REC_estimator_current = controller_obs\n",
    "        \n",
    "        REC_scheduler_current = scheduler_concatenated_est\n",
    "        REC_scheduler_current_ow = scheduler_concatenated_ow\n",
    "        \n",
    "        REC_controller_current = controller_concatenated_obs\n",
    "        REC_controller_current_ow = controller_concatenated_ow\n",
    "        \n",
    "        # communication cost    \n",
    "        if(sc_transmission == 0):\n",
    "            comm_cost_sc = 0\n",
    "        else:\n",
    "            comm_cost_sc = COMM_COST\n",
    "        \n",
    "        '''Downlink channel'''\n",
    "        if(np.random.rand() < LA):\n",
    "            action = np.zeros(CONTROL_ACTION_DIM)\n",
    "        else:\n",
    "            # Select action randomly or according to policy\n",
    "            if t < TIMESTEPS_BEFORE_TRAIN:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = (policy.select_action(np.array(controller_concatenated_ow), \n",
    "                                               np.array(controller_concatenated_obs)) \n",
    "                          + np.random.normal(0, MAX_ACTION * EXPL_NOISE_STD, \n",
    "                                             size=CONTROL_ACTION_DIM)).clip(-MAX_ACTION, MAX_ACTION)\n",
    "        \n",
    "        # Update history of transmission scheduler and intelligent controller \n",
    "        SCHEDULER_OW.add(scheduler_concatenated_est)\n",
    "        SCHEDULER_OW.add(schedule)\n",
    "        \n",
    "        CONTROLLER_OW.add(controller_concatenated_obs)\n",
    "        CONTROLLER_OW.add(action)\n",
    "        \n",
    "        # Take 1 step\n",
    "        controller_next_est = policy.predict_state(np.array(controller_obs), np.array(action))\n",
    "        \n",
    "        next_state, oc_cost, done, _ = env.step(action) \n",
    "        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "        \n",
    "        reward = oc_cost - comm_cost_sc\n",
    "        \n",
    "        q_value = policy.select_q(controller_concatenated_ow, controller_concatenated_obs, action)\n",
    "        q_reward = float(q_value)\n",
    "        \n",
    "        '''Markov fading channel state'''\n",
    "        LS = channel_state_transition(LS)\n",
    "        LA = channel_state_transition(LA) \n",
    "        \n",
    "        '''Transmission scheduling'''\n",
    "        scheduler_concatenated_est = concatenate_scheduler_state(controller_next_est, LS, AoI+1)\n",
    "        scheduler_concatenated_ow = SCHEDULER_OW.read()\n",
    "        \n",
    "        if(t < PRE_TRAINING_TIMESTEPS):    \n",
    "            schedule = np.array([1])\n",
    "        else:\n",
    "            # Select schedule action with e-greedy\n",
    "            if(np.random.rand() < EPSILON):\n",
    "                schedule = policy.schedule_transmission(np.array(scheduler_concatenated_ow), \n",
    "                                                        np.array(scheduler_concatenated_est))\n",
    "            else:\n",
    "                schedule = np.array([np.random.randint(0, 2)])\n",
    "        \n",
    "        sc_transmission = policy.control_transmission(schedule)\n",
    "        \n",
    "        '''Next time step'''\n",
    "        REC_scheduler_next = scheduler_concatenated_est\n",
    "        REC_scheduler_next_ow = scheduler_concatenated_ow\n",
    "        \n",
    "        if(np.random.rand() < LS) or (sc_transmission == 0):\n",
    "            # Predict state\n",
    "            controller_next_est = controller_next_est\n",
    "            \n",
    "            signal_sc = 0\n",
    "            AoI += 1\n",
    "            \n",
    "            REC_estimator_next = controller_next_est\n",
    "            REC_controller_next = concatenate_controller_state(controller_next_est, LS, LA, 1)\n",
    "            REC_controller_next_ow = CONTROLLER_OW.read()\n",
    "            \n",
    "            # Store transition in replay buffer\n",
    "            scheduler_experience = Experience_OW(REC_scheduler_current_ow, REC_scheduler_current, schedule, [q_reward], \n",
    "                                       REC_scheduler_next_ow, REC_scheduler_next, [1. - done_bool])\n",
    "            controller_experience = Experience_OW(REC_controller_current_ow, REC_controller_current, action, [reward], \n",
    "                                          REC_controller_next_ow, REC_controller_next, [1. - done_bool])\n",
    "            \n",
    "            aoi_label = 1\n",
    "            scheduler_replay_buffer.add(scheduler_experience, aoi_label) # transition AOI: 0/1\n",
    "            controller_replay_buffer.add(controller_experience, aoi_label)\n",
    "        else:\n",
    "            # Receive noisy observation\n",
    "            controller_next_obs = add_gaussian_noise(next_state)\n",
    "            \n",
    "            REC_estimator_next = controller_next_obs\n",
    "            REC_controller_next = concatenate_controller_state(controller_next_obs, LS, LA, 0)\n",
    "            REC_controller_next_ow = CONTROLLER_OW.read()\n",
    "            \n",
    "            # Store transition in replay buffers\n",
    "            estimator_experience = Experience(REC_estimator_current, action, [reward], \n",
    "                                              REC_estimator_next, [1. - done_bool])\n",
    "            \n",
    "            scheduler_experience = Experience_OW(REC_scheduler_current_ow, REC_scheduler_current, schedule, [q_reward], \n",
    "                                       REC_scheduler_next_ow, REC_scheduler_next, [1. - done_bool])\n",
    "            controller_experience = Experience_OW(REC_controller_current_ow, REC_controller_current, action, [reward], \n",
    "                                          REC_controller_next_ow, REC_controller_next, [1. - done_bool])\n",
    "            \n",
    "            aoi_label = 0\n",
    "            estimator_replay_buffer.add(estimator_experience, aoi_label) # transition AOI: 0/0 \n",
    "            scheduler_replay_buffer.add(scheduler_experience, aoi_label) \n",
    "            controller_replay_buffer.add(controller_experience, aoi_label)\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    # Train agent after collecting sufficient data\n",
    "    if t >= TIMESTEPS_BEFORE_TRAIN:\n",
    "        policy.train(estimator_replay_buffer, scheduler_replay_buffer, controller_replay_buffer)\n",
    "\n",
    "    if done: \n",
    "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "        \n",
    "        '''Reset variables'''\n",
    "        state, done = env.reset(), False\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1 \n",
    "        \n",
    "        INITIAL_OBS = add_gaussian_noise(state)\n",
    "        \n",
    "        controller_obs = None\n",
    "        controller_est = None\n",
    "        \n",
    "        controller_next_obs = INITIAL_OBS\n",
    "        controller_next_est = None\n",
    "        \n",
    "        LS = 0.05\n",
    "        LA = 0.05\n",
    "        \n",
    "        signal_sc = 1\n",
    "        AoI = 0\n",
    "        \n",
    "        SCHEDULER_OW.__init__()\n",
    "        CONTROLLER_OW.__init__()\n",
    "        \n",
    "        scheduler_concatenated_est = concatenate_scheduler_state(INITIAL_OBS, LS, 1)\n",
    "        scheduler_concatenated_ow = SCHEDULER_OW.read()\n",
    "        schedule = np.array([1]) \n",
    "        sc_transmission = 1\n",
    "        \n",
    "        REC_estimator_current = None\n",
    "        REC_estimator_next = None\n",
    "        \n",
    "        REC_scheduler_current = None\n",
    "        REC_scheduler_next = None\n",
    "\n",
    "        REC_scheduler_current_ow = None\n",
    "        REC_scheduler_next_ow = None\n",
    "\n",
    "        REC_controller_current = None\n",
    "        REC_controller_next = None\n",
    "\n",
    "        REC_controller_current_ow = None\n",
    "        REC_controller_next_ow = None\n",
    "\n",
    "    '''Evaluate trained NNs'''\n",
    "    if (t + 1) % EVAL_FREQ == 0:\n",
    "        eval_policy(policy, ENV_NAME, SEED, t + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
